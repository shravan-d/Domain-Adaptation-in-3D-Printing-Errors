{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3c41abd9-e91c-4570-a934-12063884df3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms\n",
    "import numpy as np\n",
    "from torchvision.datasets import MNIST, SVHN\n",
    "from torch.utils.data import DataLoader\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from matplotlib import pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import torchvision\n",
    "import torchvision.models as models\n",
    "import datetime\n",
    "import sys\n",
    "import sklearn\n",
    "from torch.autograd import Function\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import f1_score\n",
    "import os\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "from torch.autograd import Variable\n",
    "from torchvision import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "d24dcbfa-7d95-43f7-8e97-73713c067909",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU\n"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "lr = 0.03\n",
    "momentum = 0.5\n",
    "seed = 40\n",
    "cuda = True\n",
    "log_interval = 600\n",
    "random = False\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.backends.cudnn.deterministic = True\n",
    "batch_size = 16\n",
    "input_size = (128, 64)\n",
    "aspect_ratio = [2, 1]\n",
    "is_vit = False\n",
    "if(torch.cuda.is_available()):\n",
    "  device=torch.device('cuda')\n",
    "  print(\"GPU\")\n",
    "else:\n",
    "  device=torch.device('cpu')\n",
    "  print('CPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7db92b82-f255-4461-8775-b959aee38640",
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize_image_with_aspect(image):\n",
    "    w, h = image.size\n",
    "    cut_h = h - aspect_ratio[1] * w / aspect_ratio[0]\n",
    "    image = image.crop((0, cut_h / 2, w, h-(cut_h/2)))\n",
    "    image = image.resize(input_size)\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e7b7065a-99bf-4f55-b1f1-5e4916abb38d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_coeff(iter_num, high=1.0, low=0.0, alpha=10.0, max_iter=10000.0):\n",
    "    return float(2.0 * (high - low) / (1.0 + np.exp(-alpha * iter_num / max_iter)) - (high - low) + low)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "721eced9-adfa-4ce9-9136-9fa694c75757",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv2d') != -1 or classname.find('ConvTranspose2d') != -1:\n",
    "        nn.init.kaiming_uniform_(m.weight)\n",
    "        nn.init.zeros_(m.bias)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        nn.init.normal_(m.weight, 1.0, 0.02)\n",
    "        nn.init.zeros_(m.bias)\n",
    "    elif classname.find('Linear') != -1:\n",
    "        nn.init.xavier_normal_(m.weight)\n",
    "        # nn.init.zeros_(m.bias)\n",
    "        m.bias.data.fill_(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "2d40b0fb-375b-4013-98e1-68185f56e3dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Entropy(input_):\n",
    "    epsilon = 1e-5\n",
    "    entropy = -input_ * torch.log(input_ + epsilon)\n",
    "    entropy = torch.sum(entropy, dim=1)\n",
    "    return entropy\n",
    "\n",
    "\n",
    "def grl_hook(coeff):\n",
    "    def fun1(grad):\n",
    "        return -coeff * grad.clone()\n",
    "\n",
    "    return fun1\n",
    "\n",
    "\n",
    "def CDAN(input_list, ad_net, entropy=None, coeff=None, random_layer=None):\n",
    "    softmax_output = input_list[1].detach()\n",
    "    feature = input_list[0]\n",
    "    if random_layer is None:\n",
    "        op_out = torch.bmm(softmax_output.unsqueeze(2), feature.unsqueeze(1))\n",
    "        ad_out = ad_net(op_out.view(-1, softmax_output.size(1) * feature.size(1)))\n",
    "    else:\n",
    "        random_out = random_layer.forward([feature, softmax_output])\n",
    "        ad_out = ad_net(random_out.view(-1, random_out.size(1)))\n",
    "    batch_size = softmax_output.size(0) // 2\n",
    "    dc_target = torch.from_numpy(np.array([[1]] * batch_size + [[0]] * batch_size)).float()\n",
    "    if cuda:\n",
    "        dc_target = dc_target.cuda()\n",
    "    if entropy is not None:\n",
    "        entropy.register_hook(grl_hook(coeff))\n",
    "        entropy = 1.0 + torch.exp(-entropy)\n",
    "        source_mask = torch.ones_like(entropy)\n",
    "        source_mask[feature.size(0) // 2:] = 0\n",
    "        source_weight = entropy * source_mask\n",
    "        target_mask = torch.ones_like(entropy)\n",
    "        target_mask[0:feature.size(0) // 2] = 0\n",
    "        target_weight = entropy * target_mask\n",
    "        weight = source_weight / torch.sum(source_weight).detach().item() + \\\n",
    "                 target_weight / torch.sum(target_weight).detach().item()\n",
    "        l = nn.BCELoss(reduction='none')(ad_out, dc_target)\n",
    "        return torch.sum(weight.view(-1, 1) * nn.BCELoss()(ad_out, dc_target)) / torch.sum(weight).detach().item()\n",
    "    else:\n",
    "        return nn.BCELoss()(ad_out, dc_target)\n",
    "\n",
    "\n",
    "def mdd_loss(features, labels, left_weight=1, right_weight=1):\n",
    "    softmax_out = nn.Softmax(dim=1)(features)\n",
    "    batch_size = features.size(0)\n",
    "    if float(batch_size) % 2 != 0:\n",
    "        raise Exception('Incorrect batch size provided')\n",
    "\n",
    "    batch_left = softmax_out[:int(0.5 * batch_size)]\n",
    "    batch_right = softmax_out[int(0.5 * batch_size):]\n",
    "\n",
    "    loss = torch.norm((batch_left - batch_right).abs(), 2, 1).sum() / float(batch_size)\n",
    "\n",
    "    labels_left = labels[:int(0.5 * batch_size)]\n",
    "    batch_left_loss = get_pari_loss1(labels_left, batch_left)\n",
    "\n",
    "    labels_right = labels[int(0.5 * batch_size):]\n",
    "    batch_right_loss = get_pari_loss1(labels_right, batch_right)\n",
    "    return loss + left_weight * batch_left_loss + right_weight * batch_right_loss\n",
    "\n",
    "\n",
    "def mdd_digit(features, labels, left_weight=1, right_weight=1, weight=1):\n",
    "    softmax_out = nn.Softmax(dim=1)(features)\n",
    "    batch_size = features.size(0)\n",
    "    if float(batch_size) % 2 != 0:\n",
    "        raise Exception('Incorrect batch size provided')\n",
    "\n",
    "    batch_left = softmax_out[:int(0.5 * batch_size)]\n",
    "    batch_right = softmax_out[int(0.5 * batch_size):]\n",
    "\n",
    "    loss = torch.norm((batch_left - batch_right).abs(), 2, 1).sum() / float(batch_size)\n",
    "\n",
    "    labels_left = labels[:int(0.5 * batch_size)]\n",
    "    labels_left_left = labels_left[:int(0.25 * batch_size)]\n",
    "    labels_left_right = labels_left[int(0.25 * batch_size):]\n",
    "\n",
    "    batch_left_left = batch_left[:int(0.25 * batch_size)]\n",
    "    batch_left_right = batch_left[int(0.25 * batch_size):]\n",
    "    batch_left_loss = get_pair_loss(labels_left_left, labels_left_right, batch_left_left, batch_left_right)\n",
    "\n",
    "    labels_right = labels[int(0.5 * batch_size):]\n",
    "    labels_right_left = labels_right[:int(0.25 * batch_size)]\n",
    "    labels_right_right = labels_right[int(0.25 * batch_size):]\n",
    "\n",
    "    batch_right_left = batch_right[:int(0.25 * batch_size)]\n",
    "    batch_right_right = batch_right[int(0.25 * batch_size):]\n",
    "    batch_right_loss = get_pair_loss(labels_right_left, labels_right_right, batch_right_left, batch_right_right)\n",
    "\n",
    "    return weight*loss + left_weight * batch_left_loss + right_weight * batch_right_loss\n",
    "\n",
    "def get_pair_loss(labels_left, labels_right, features_left, features_right):\n",
    "    loss = 0\n",
    "    for i in range(len(labels_left)):\n",
    "        if (labels_left[i] == labels_right[i]):\n",
    "            loss += torch.norm((features_left[i] - features_right[i]).abs(), 2, 0).sum()\n",
    "    return loss\n",
    "\n",
    "def get_pari_loss1(labels, features):\n",
    "    loss = 0\n",
    "    count = 0\n",
    "    for i in range(len(labels)):\n",
    "        for j in range(i + 1, len(labels)):\n",
    "            if (labels[i] == labels[j]):\n",
    "                count += 1\n",
    "                loss += torch.norm((features[i] - features[j]).abs(), 2, 0).sum()\n",
    "    return loss / count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "abf83534-4899-4c2f-b3f6-098817647567",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.43335001, 0.43360192, 0.42602362), (0.28486016, 0.28320433, 0.28699529)),\n",
    "])\n",
    "flipTransform = transforms.RandomHorizontalFlip(p=1)\n",
    "zoomTransform = transforms.RandomResizedCrop(input_size[:: -1], scale=(0.7, 1))\n",
    "colorTransform = transforms.ColorJitter(brightness=(0.65, 0.9), contrast=(1.1, 1.35))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cb65450d-b204-4952-83f1-fb418bc55deb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_batches(df, train=True, validation=False):\n",
    "    if train:\n",
    "        labels = df['has_under_extrusion'].tolist()\n",
    "    printers = df['printer_id'].unique().tolist()\n",
    "    printer_domain_map = {printer_id: idx for idx, printer_id in enumerate(printers)}\n",
    "    domains = [printer_domain_map[domain] for domain in df['printer_id'].tolist()]\n",
    "    prints = df['print_id'].unique().tolist()\n",
    "    print_domain_map = {print_id: idx for idx, print_id in enumerate(prints)}\n",
    "    print_jobs = [print_domain_map[print] for print in df['print_id'].tolist()]\n",
    "        \n",
    "    image_paths = df['img_path'].tolist()\n",
    "    current = 0\n",
    "    while current < len(df):\n",
    "        batch_images, batch_domains, batch_labels, batch_paths, batch_prints = [], [], [], [], []\n",
    "        batch_idx = 0\n",
    "        reserve_images, reserve_domains, reserve_labels, reserve_prints = [], [], [], []\n",
    "        \n",
    "        while batch_idx < batch_size:\n",
    "            if current + batch_idx >= len(df):\n",
    "                break\n",
    "            \n",
    "            image_path = image_paths[current + batch_idx]\n",
    "            domain = domains[current + batch_idx]\n",
    "            print_job = print_jobs[current + batch_idx]\n",
    "            image_path = image_path.split('/')[-1]\n",
    "            filename = '/home/shravandinakaran/data/'+image_path\n",
    "            image = Image.open(filename)\n",
    "            if is_vit:\n",
    "                image = resize_to_square(image)\n",
    "            else:\n",
    "                image = resize_image_with_aspect(image)\n",
    "            if train:\n",
    "                label = labels[current + batch_idx]\n",
    "                batch_labels.append(label)\n",
    "            batch_domains.append(domain)\n",
    "            batch_prints.append(print_job)\n",
    "            batch_images.append(transform(image))\n",
    "            batch_paths.append(image_path)\n",
    "            batch_idx += 1\n",
    "        current += batch_size\n",
    "        if train:\n",
    "            yield batch_images, np.array(batch_labels), np.array(batch_domains), np.array(batch_prints)\n",
    "        else:\n",
    "            yield batch_images, batch_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cbff2b5e-2b95-42d5-b1cc-e61588a88947",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35900 40713\n"
     ]
    }
   ],
   "source": [
    "train_df = pd.read_csv('/home/shravandinakaran/data/train_.csv')\n",
    "test_df = pd.read_csv('/home/shravandinakaran/data/test_.csv')\n",
    "val_df = pd.read_csv('/home/shravandinakaran/data/val_.csv')\n",
    "test_df = test_df.append(val_df)\n",
    "rows = train_df.loc[train_df['print_id'].isin([1678419624, 1678445757, 1679265796, 1679221334, 1678589738])]\n",
    "test_df = test_df.append(rows, ignore_index=True)\n",
    "train_df.drop(rows.index, inplace=True)\n",
    "print(len(test_df), len(train_df))\n",
    "num_domains_train = len(train_df['printer_id'].unique().tolist())\n",
    "num_domains_test = len(test_df['printer_id'].unique().tolist())\n",
    "num_domains_total = len(set(train_df['printer_id'].unique().tolist() + test_df['printer_id'].unique().tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "b229c116-2983-4fba-b580-250ed156ba82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30162 30693\n"
     ]
    }
   ],
   "source": [
    "source_df = pd.read_csv('/home/shravandinakaran/data/source.csv')\n",
    "target_df = pd.read_csv('/home/shravandinakaran/data/target.csv')\n",
    "print(len(source_df), len(target_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c7c93bc3-346e-4681-83ff-1126c5fb9057",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Identity(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Identity, self).__init__()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "45bc08cc-b121-4848-aa32-8cc8583adf0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReverseLayerF(Function):\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(ctx, x, alpha):\n",
    "        ctx.alpha = alpha\n",
    "\n",
    "        return x.view_as(x)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        output = grad_output.neg() * ctx.alpha\n",
    "\n",
    "        return output, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8c72444c-b61b-40bd-a39a-5e2d7ed123ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DTN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DTN, self).__init__()\n",
    "        self.conv_params = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=5, stride=2, padding=2),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.Dropout2d(0.1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 128, kernel_size=5, stride=2, padding=2),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.Dropout2d(0.3),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(128, 256, kernel_size=5, stride=2, padding=2),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.Dropout2d(0.5),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        self.fc_params = nn.Sequential(\n",
    "            nn.Linear(256 * 8 * 16, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout()\n",
    "        )\n",
    "\n",
    "        self.classifier = nn.Linear(512, 2)\n",
    "        self.__in_features = 512\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_params(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc_params(x)\n",
    "        y = self.classifier(x)\n",
    "        return x, y\n",
    "\n",
    "    def output_num(self):\n",
    "        return self.__in_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4d1cf528-9c10-4d4b-babb-2395fe318012",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet18(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ResNet18, self).__init__()\n",
    "        self.resnet = models.resnet18()\n",
    "        self.resnet.fc = Identity()\n",
    "        \n",
    "        self.fc_params = nn.Sequential(\n",
    "            nn.Linear(512, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout()\n",
    "        )\n",
    "\n",
    "        self.classifier = nn.Linear(512, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.resnet(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc_params(x)\n",
    "        y = self.classifier(x)\n",
    "        return x, y\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5675b5c6-232c-4b5b-82b8-d0fb995929bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class AdversarialNetwork(nn.Module):\n",
    "    def __init__(self, in_feature, hidden_size):\n",
    "        super(AdversarialNetwork, self).__init__()\n",
    "        self.ad_layer1 = nn.Linear(in_feature, hidden_size)\n",
    "        self.ad_layer2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.ad_layer3 = nn.Linear(hidden_size, 1)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.dropout1 = nn.Dropout(0.5)\n",
    "        self.dropout2 = nn.Dropout(0.5)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.apply(init_weights)\n",
    "        self.iter_num = 0\n",
    "        self.alpha = 10\n",
    "        self.low = 0.0\n",
    "        self.high = 1.0\n",
    "        self.max_iter = 10000.0\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.training:\n",
    "            self.iter_num += 1\n",
    "        coeff = calc_coeff(self.iter_num, self.high, self.low, self.alpha, self.max_iter)\n",
    "        x = x * 1.0\n",
    "        x.register_hook(grl_hook(coeff))\n",
    "        x = self.ad_layer1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.dropout1(x)\n",
    "        x = self.ad_layer2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.dropout2(x)\n",
    "        y = self.ad_layer3(x)\n",
    "        y = self.sigmoid(y)\n",
    "        return y\n",
    "\n",
    "    def output_num(self):\n",
    "        return 1\n",
    "\n",
    "    def get_parameters(self):\n",
    "        return [{\"params\": self.parameters(), \"lr_mult\": 10, 'decay_mult': 2}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "b213afe8-6b36-4b5d-93cc-c50c3efe2abb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(epoch, model, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    for data, target, _, _ in test_loader:\n",
    "        data = torch.stack(data, dim=0, out=None).to(device)\n",
    "        target = torch.from_numpy(target).to(device)\n",
    "        feature, output = model(data)\n",
    "        test_loss += nn.CrossEntropyLoss()(output, target).item()\n",
    "        pred = output.data.cpu().max(1, keepdim=True)[1]\n",
    "        correct += pred.eq(target.data.cpu().view_as(pred)).sum().item()\n",
    "        \n",
    "    test_loss /= len(target_df)\n",
    "    acc = 100. * correct / len(target_df)\n",
    "    log_str = 'epoch:{},Test set: Average loss: {:.4f}, Accuracy: {}/{} ({:.4f}%)\\n'.format(epoch,\n",
    "                                                                                            test_loss, correct,\n",
    "                                                                                            len(target_df),\n",
    "                                                                                            acc)\n",
    "\n",
    "    print(log_str)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "46191d80-b01c-48ef-aae6-15717baf3181",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ResNet18() #DTN()\n",
    "if cuda:\n",
    "    model = model.cuda()\n",
    "class_num = 2\n",
    "\n",
    "if random:\n",
    "    random_layer = network.RandomLayer([model.output_num(), class_num], 500)\n",
    "    ad_net = AdversarialNetwork(500, 500)\n",
    "    if cuda:\n",
    "        random_layer.cuda()\n",
    "else:\n",
    "    random_layer = None\n",
    "    ad_net = AdversarialNetwork(512 * class_num, 500)\n",
    "if cuda:\n",
    "    ad_net = ad_net.cuda()\n",
    "optimizer = optim.SGD(model.parameters(), lr=lr, weight_decay=0.0005, momentum=0.9)\n",
    "optimizer_ad = optim.SGD(ad_net.parameters(), lr=lr, weight_decay=0.0005, momentum=0.9)\n",
    "\n",
    "CELoss = nn.CrossEntropyLoss()\n",
    "Softmax = nn.Softmax(dim=1)\n",
    "mdd_weight = 1\n",
    "\n",
    "best_model = model\n",
    "best_acc = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3673765-dd3c-45fa-94df-612ccf057b0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [8240/30160 (27%)]\tLoss: 0.673\n",
      "Train Epoch: 1 [17840/30160 (59%)]\tLoss: 0.683\n",
      "Train Epoch: 1 [27440/30160 (91%)]\tLoss: 0.705\n",
      "train_loss:0.752, accuracy:98.873\n",
      "\n",
      "epoch:1,Test set: Average loss: 0.3793, Accuracy: 12892/30693 (42.0031%)\n",
      "\n",
      "Best Acc so far:  42.003062587560684\n",
      "Train Epoch: 2 [6880/30160 (23%)]\tLoss: 0.705\n",
      "Train Epoch: 2 [16480/30160 (55%)]\tLoss: 0.721\n",
      "Train Epoch: 2 [26080/30160 (86%)]\tLoss: 0.690\n",
      "train_loss:0.701, accuracy:99.831\n",
      "\n",
      "epoch:2,Test set: Average loss: 0.3626, Accuracy: 14374/30693 (46.8315%)\n",
      "\n",
      "Best Acc so far:  46.83152510344378\n",
      "Train Epoch: 3 [5520/30160 (18%)]\tLoss: 0.703\n",
      "Train Epoch: 3 [15120/30160 (50%)]\tLoss: 0.679\n",
      "Train Epoch: 3 [24720/30160 (82%)]\tLoss: 0.714\n",
      "train_loss:0.693, accuracy:99.910\n",
      "\n",
      "epoch:3,Test set: Average loss: 0.3795, Accuracy: 12697/30693 (41.3677%)\n",
      "\n",
      "Train Epoch: 4 [4160/30160 (14%)]\tLoss: 0.720\n",
      "Train Epoch: 4 [13760/30160 (46%)]\tLoss: 0.690\n",
      "Train Epoch: 4 [23360/30160 (77%)]\tLoss: 0.684\n",
      "train_loss:0.688, accuracy:99.980\n",
      "\n",
      "epoch:4,Test set: Average loss: 0.3810, Accuracy: 12615/30693 (41.1006%)\n",
      "\n",
      "Best Acc so far:  46.83152510344378\n",
      "Train Epoch: 5 [2800/30160 (9%)]\tLoss: 0.743\n",
      "Train Epoch: 5 [12400/30160 (41%)]\tLoss: 0.659\n",
      "Train Epoch: 5 [22000/30160 (73%)]\tLoss: 0.698\n",
      "train_loss:0.692, accuracy:99.950\n",
      "\n",
      "epoch:5,Test set: Average loss: 0.3979, Accuracy: 14049/30693 (45.7727%)\n",
      "\n",
      "Best Acc so far:  46.83152510344378\n",
      "Train Epoch: 6 [1440/30160 (5%)]\tLoss: 0.694\n",
      "Train Epoch: 6 [11040/30160 (37%)]\tLoss: 0.676\n",
      "Train Epoch: 6 [20640/30160 (68%)]\tLoss: 0.693\n",
      "train_loss:0.693, accuracy:99.987\n",
      "\n",
      "epoch:6,Test set: Average loss: 0.3891, Accuracy: 14048/30693 (45.7694%)\n",
      "\n",
      "Train Epoch: 7 [80/30160 (0%)]\tLoss: 0.686\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, epochs + 1):\n",
    "    if epoch % 3 == 0:\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group[\"lr\"] = param_group[\"lr\"] * 0.3\n",
    "            \n",
    "    source_df = source_df.sample(frac=1).reset_index(drop=True)\n",
    "    target_df = target_df.sample(frac=1).reset_index(drop=True)\n",
    "    source_generator = generate_batches(source_df)\n",
    "    target_generator = generate_batches(target_df)\n",
    "    \n",
    "    model.train()\n",
    "    len_source = len(source_df)\n",
    "    len_target = len(target_df)\n",
    "    num_iter = min(len_source // batch_size, len_target // batch_size)\n",
    "    total_loss, correct = 0, 0\n",
    "    for batch_idx in range(num_iter):\n",
    "        data_source, label_source, _, _ = next(source_generator)\n",
    "        data_source = torch.stack(data_source, dim=0, out=None).to(device)\n",
    "        label_source = torch.from_numpy(label_source).to(device)\n",
    "        \n",
    "        data_target, label_target, _, _ = next(target_generator)\n",
    "        data_target = torch.stack(data_target, dim=0, out=None).to(device)\n",
    "        label_target = torch.from_numpy(label_target).to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        optimizer_ad.zero_grad()\n",
    "\n",
    "        feature_source, output_source = model(data_source)\n",
    "        feature_target, output_target = model(data_target)\n",
    "\n",
    "        feature = torch.cat((feature_source, feature_target), 0)\n",
    "        output = torch.cat((output_source, output_target), 0)\n",
    "\n",
    "        labels_target_fake = torch.max(nn.Softmax(dim=1)(output_target), 1)[1]\n",
    "        labels = torch.cat((label_source, labels_target_fake))\n",
    "\n",
    "        loss = CELoss(output.narrow(0, 0, data_source.size(0)), label_source)\n",
    "\n",
    "        softmax_output = Softmax(output)\n",
    "        if epoch > 0:\n",
    "            entropy = Entropy(softmax_output)\n",
    "            loss += CDAN([feature, softmax_output], ad_net, entropy,\n",
    "                                   calc_coeff(num_iter * (epoch - 0) + batch_idx), random_layer)\n",
    "\n",
    "        mdd_loss = 0.0 #mdd_weight * mdd_loss(feature, labels)\n",
    "        loss = loss + mdd_loss\n",
    "\n",
    "        total_loss += loss.data\n",
    "        \n",
    "        pred = output_source.data.cpu().max(1, keepdim=True)[1]\n",
    "        correct += pred.eq(label_source.data.cpu().view_as(pred)).sum().item()\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if epoch > 0:\n",
    "            optimizer_ad.step()\n",
    "        if (batch_idx + epoch * num_iter) % log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.3f}'.format(\n",
    "                epoch, batch_idx * batch_size, num_iter * batch_size,\n",
    "                       100. * batch_idx / num_iter, loss.item()))\n",
    "    \n",
    "    log_str = \"train_loss:{:.3f}, accuracy:{:.3f}\\n\".format(total_loss / num_iter, 100*correct / len(source_df))\n",
    "    print(log_str)\n",
    "    \n",
    "    target_generator = generate_batches(target_df)\n",
    "    acc = test(epoch, model, target_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "0c37b372-2bfe-44eb-82f8-1fff72170fe5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:2,Test set: Average loss: 0.2358, Accuracy: 11900/30693 (38.7711%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "target_generator = generate_batches(target_df)\n",
    "acc = test(epoch, model, target_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82dead4e-3fb7-43db-8c46-b9e854aeb0b0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "pytorch-gpu.1-13.m112",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.1-13:m112"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
