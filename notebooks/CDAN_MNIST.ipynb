{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":7084545,"sourceType":"datasetVersion","datasetId":3975596}],"dockerImageVersionId":30588,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import argparse\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torchvision import transforms\nimport os\nimport numpy as np\nfrom torchvision.datasets import MNIST, SVHN\nfrom torch.utils.data import DataLoader\nfrom matplotlib import pyplot as plt","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-11-30T17:54:39.794081Z","iopub.execute_input":"2023-11-30T17:54:39.794455Z","iopub.status.idle":"2023-11-30T17:54:43.254903Z","shell.execute_reply.started":"2023-11-30T17:54:39.794424Z","shell.execute_reply":"2023-11-30T17:54:43.253806Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"batch_size = 64\ntest_batch_size = 1000\nepochs = 100\nlr = 0.03\nmomentum = 0.5\nseed = 40\ncuda = True\nlog_interval = 100\nrandom = False\ntorch.backends.cudnn.benchmark = False\ntorch.backends.cudnn.deterministic = True","metadata":{"execution":{"iopub.status.busy":"2023-11-30T17:57:09.100340Z","iopub.execute_input":"2023-11-30T17:57:09.100772Z","iopub.status.idle":"2023-11-30T17:57:09.106949Z","shell.execute_reply.started":"2023-11-30T17:57:09.100741Z","shell.execute_reply":"2023-11-30T17:57:09.105614Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def calc_coeff(iter_num, high=1.0, low=0.0, alpha=10.0, max_iter=10000.0):\n    return float(2.0 * (high - low) / (1.0 + np.exp(-alpha * iter_num / max_iter)) - (high - low) + low)","metadata":{"execution":{"iopub.status.busy":"2023-11-30T17:54:45.393608Z","iopub.execute_input":"2023-11-30T17:54:45.394020Z","iopub.status.idle":"2023-11-30T17:54:45.399714Z","shell.execute_reply.started":"2023-11-30T17:54:45.393987Z","shell.execute_reply":"2023-11-30T17:54:45.398635Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def init_weights(m):\n    classname = m.__class__.__name__\n    if classname.find('Conv2d') != -1 or classname.find('ConvTranspose2d') != -1:\n        nn.init.kaiming_uniform_(m.weight)\n        nn.init.zeros_(m.bias)\n    elif classname.find('BatchNorm') != -1:\n        nn.init.normal_(m.weight, 1.0, 0.02)\n        nn.init.zeros_(m.bias)\n    elif classname.find('Linear') != -1:\n        nn.init.xavier_normal_(m.weight)\n        # nn.init.zeros_(m.bias)\n        m.bias.data.fill_(0)","metadata":{"execution":{"iopub.status.busy":"2023-11-30T17:54:45.739729Z","iopub.execute_input":"2023-11-30T17:54:45.740586Z","iopub.status.idle":"2023-11-30T17:54:45.747575Z","shell.execute_reply.started":"2023-11-30T17:54:45.740553Z","shell.execute_reply":"2023-11-30T17:54:45.746576Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def Entropy(input_):\n    epsilon = 1e-5\n    entropy = -input_ * torch.log(input_ + epsilon)\n    entropy = torch.sum(entropy, dim=1)\n    return entropy\n\n\ndef grl_hook(coeff):\n    def fun1(grad):\n        return -coeff * grad.clone()\n\n    return fun1\n\n\ndef CDAN(input_list, ad_net, entropy=None, coeff=None, random_layer=None):\n    softmax_output = input_list[1].detach()\n    feature = input_list[0]\n    if random_layer is None:\n        op_out = torch.bmm(softmax_output.unsqueeze(2), feature.unsqueeze(1))\n        ad_out = ad_net(op_out.view(-1, softmax_output.size(1) * feature.size(1)))\n    else:\n        random_out = random_layer.forward([feature, softmax_output])\n        ad_out = ad_net(random_out.view(-1, random_out.size(1)))\n    batch_size = softmax_output.size(0) // 2\n    dc_target = torch.from_numpy(np.array([[1]] * batch_size + [[0]] * batch_size)).float()\n    if cuda:\n        dc_target = dc_target.cuda()\n    if entropy is not None:\n        entropy.register_hook(grl_hook(coeff))\n        entropy = 1.0 + torch.exp(-entropy)\n        source_mask = torch.ones_like(entropy)\n        source_mask[feature.size(0) // 2:] = 0\n        source_weight = entropy * source_mask\n        target_mask = torch.ones_like(entropy)\n        target_mask[0:feature.size(0) // 2] = 0\n        target_weight = entropy * target_mask\n        weight = source_weight / torch.sum(source_weight).detach().item() + \\\n                 target_weight / torch.sum(target_weight).detach().item()\n        l = nn.BCELoss(reduction='none')(ad_out, dc_target)\n        return torch.sum(weight.view(-1, 1) * nn.BCELoss()(ad_out, dc_target)) / torch.sum(weight).detach().item()\n    else:\n        return nn.BCELoss()(ad_out, dc_target)\n\n\ndef mdd_loss(features, labels, left_weight=1, right_weight=1):\n    softmax_out = nn.Softmax(dim=1)(features)\n    batch_size = features.size(0)\n    if float(batch_size) % 2 != 0:\n        raise Exception('Incorrect batch size provided')\n\n    batch_left = softmax_out[:int(0.5 * batch_size)]\n    batch_right = softmax_out[int(0.5 * batch_size):]\n\n    loss = torch.norm((batch_left - batch_right).abs(), 2, 1).sum() / float(batch_size)\n\n    labels_left = labels[:int(0.5 * batch_size)]\n    batch_left_loss = get_pari_loss1(labels_left, batch_left)\n\n    labels_right = labels[int(0.5 * batch_size):]\n    batch_right_loss = get_pari_loss1(labels_right, batch_right)\n    return loss + left_weight * batch_left_loss + right_weight * batch_right_loss\n\n\ndef mdd_digit(features, labels, left_weight=1, right_weight=1, weight=1):\n    softmax_out = nn.Softmax(dim=1)(features)\n    batch_size = features.size(0)\n    if float(batch_size) % 2 != 0:\n        raise Exception('Incorrect batch size provided')\n\n    batch_left = softmax_out[:int(0.5 * batch_size)]\n    batch_right = softmax_out[int(0.5 * batch_size):]\n\n    loss = torch.norm((batch_left - batch_right).abs(), 2, 1).sum() / float(batch_size)\n\n    labels_left = labels[:int(0.5 * batch_size)]\n    labels_left_left = labels_left[:int(0.25 * batch_size)]\n    labels_left_right = labels_left[int(0.25 * batch_size):]\n\n    batch_left_left = batch_left[:int(0.25 * batch_size)]\n    batch_left_right = batch_left[int(0.25 * batch_size):]\n    batch_left_loss = get_pair_loss(labels_left_left, labels_left_right, batch_left_left, batch_left_right)\n\n    labels_right = labels[int(0.5 * batch_size):]\n    labels_right_left = labels_right[:int(0.25 * batch_size)]\n    labels_right_right = labels_right[int(0.25 * batch_size):]\n\n    batch_right_left = batch_right[:int(0.25 * batch_size)]\n    batch_right_right = batch_right[int(0.25 * batch_size):]\n    batch_right_loss = get_pair_loss(labels_right_left, labels_right_right, batch_right_left, batch_right_right)\n\n    return weight*loss + left_weight * batch_left_loss + right_weight * batch_right_loss\n","metadata":{"execution":{"iopub.status.busy":"2023-11-30T17:54:46.266583Z","iopub.execute_input":"2023-11-30T17:54:46.267109Z","iopub.status.idle":"2023-11-30T17:54:46.291335Z","shell.execute_reply.started":"2023-11-30T17:54:46.267060Z","shell.execute_reply":"2023-11-30T17:54:46.290293Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def normalize(data_tensor):\n    '''re-scale image values to [-1, 1]'''\n    return (data_tensor / 255.) * 2. - 1. \n\ndef tile_image(image):\n    print(np.array(image).max(), np.array(image).min())\n    return image #image.repeat(3,1,1)\n\ntransform_list = [transforms.ToTensor(), transforms.Lambda(lambda x: normalize(x))]\n\nmnist_dataset = MNIST(root=\"data\", train=True, download=True,\n    transform=transforms.Compose([         \n        transforms.Grayscale(3),\n        transforms.Resize(32),\n        transforms.ToTensor(),\n        transforms.Normalize([0.1307, 0.1307, 0.1307], [0.3081, 0.3081, 0.3081])\n        ]))\n\nmnist_test_dataset = MNIST(root=\"data\", train=False, download=True,\n    transform=transforms.Compose([         \n        transforms.Grayscale(3),\n        transforms.Resize(32),\n        transforms.ToTensor(),\n        transforms.Normalize([0.1307, 0.1307, 0.1307], [0.3081, 0.3081, 0.3081])\n        ]))\n\nsvhn_dataset = SVHN(root=\"data\", split='train', download=True,\n    transform=transforms.Compose([\n        transforms.ToTensor(),\n        transforms.Normalize([0.4377, 0.4438, 0.4728], [0.1980, 0.2010, 0.1970]),\n    ]))\n\n\nsvhn_test_dataset = SVHN(root=\"data\", split='test', download=True,\n    transform=transforms.Compose([\n        transforms.ToTensor(),\n        transforms.Normalize([0.4377, 0.4438, 0.4728], [0.1980, 0.2010, 0.1970]),\n    ]))\n\nsvhn_loader = DataLoader(dataset=svhn_dataset, batch_size=batch_size, shuffle=True, drop_last=True, num_workers=0)\nsvhn_test_loader = DataLoader(dataset=svhn_test_dataset, batch_size=batch_size, shuffle=True, drop_last=True, num_workers=0)\nmnist_loader = DataLoader(dataset=mnist_dataset, batch_size=batch_size, shuffle=True, drop_last=True, num_workers=0)\nmnist_test_loader = DataLoader(dataset=mnist_test_dataset, batch_size=batch_size, shuffle=True, drop_last=True, num_workers=0)","metadata":{"execution":{"iopub.status.busy":"2023-11-30T17:54:47.849842Z","iopub.execute_input":"2023-11-30T17:54:47.850593Z","iopub.status.idle":"2023-11-30T17:55:31.707196Z","shell.execute_reply.started":"2023-11-30T17:54:47.850561Z","shell.execute_reply":"2023-11-30T17:55:31.706253Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for batch_idx, (data, target) in enumerate(mnist_loader):\n    plt.imshow(data[0].permute(1, 2, 0))\n    plt.show()\n    print(data[0].size())\n    break","metadata":{"execution":{"iopub.status.busy":"2023-11-30T17:55:35.443720Z","iopub.execute_input":"2023-11-30T17:55:35.444097Z","iopub.status.idle":"2023-11-30T17:55:35.678390Z","shell.execute_reply.started":"2023-11-30T17:55:35.444071Z","shell.execute_reply":"2023-11-30T17:55:35.677302Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class DTN(nn.Module):\n    def __init__(self):\n        super(DTN, self).__init__()\n        self.conv_params = nn.Sequential(\n            nn.Conv2d(3, 64, kernel_size=5, stride=2, padding=2),\n            nn.BatchNorm2d(64),\n            nn.Dropout2d(0.1),\n            nn.ReLU(),\n            nn.Conv2d(64, 128, kernel_size=5, stride=2, padding=2),\n            nn.BatchNorm2d(128),\n            nn.Dropout2d(0.3),\n            nn.ReLU(),\n            nn.Conv2d(128, 256, kernel_size=5, stride=2, padding=2),\n            nn.BatchNorm2d(256),\n            nn.Dropout2d(0.5),\n            nn.ReLU()\n        )\n\n        self.fc_params = nn.Sequential(\n            nn.Linear(256 * 4 * 4, 512),\n            nn.BatchNorm1d(512),\n            nn.ReLU(),\n            nn.Dropout()\n        )\n\n        self.classifier = nn.Linear(512, 10)\n        self.__in_features = 512\n\n    def forward(self, x):\n        x = self.conv_params(x)\n        x = x.view(x.size(0), -1)\n        x = self.fc_params(x)\n        y = self.classifier(x)\n        return x, y\n\n    def output_num(self):\n        return self.__in_features","metadata":{"execution":{"iopub.status.busy":"2023-11-30T17:55:41.452224Z","iopub.execute_input":"2023-11-30T17:55:41.452642Z","iopub.status.idle":"2023-11-30T17:55:41.463935Z","shell.execute_reply.started":"2023-11-30T17:55:41.452609Z","shell.execute_reply":"2023-11-30T17:55:41.462605Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nclass AdversarialNetwork(nn.Module):\n    def __init__(self, in_feature, hidden_size):\n        super(AdversarialNetwork, self).__init__()\n        self.ad_layer1 = nn.Linear(in_feature, hidden_size)\n        self.ad_layer2 = nn.Linear(hidden_size, hidden_size)\n        self.ad_layer3 = nn.Linear(hidden_size, 1)\n        self.relu1 = nn.ReLU()\n        self.relu2 = nn.ReLU()\n        self.dropout1 = nn.Dropout(0.5)\n        self.dropout2 = nn.Dropout(0.5)\n        self.sigmoid = nn.Sigmoid()\n        self.apply(init_weights)\n        self.iter_num = 0\n        self.alpha = 10\n        self.low = 0.0\n        self.high = 1.0\n        self.max_iter = 10000.0\n\n    def forward(self, x):\n        if self.training:\n            self.iter_num += 1\n        coeff = calc_coeff(self.iter_num, self.high, self.low, self.alpha, self.max_iter)\n        x = x * 1.0\n        x.register_hook(grl_hook(coeff))\n        x = self.ad_layer1(x)\n        x = self.relu1(x)\n        x = self.dropout1(x)\n        x = self.ad_layer2(x)\n        x = self.relu2(x)\n        x = self.dropout2(x)\n        y = self.ad_layer3(x)\n        y = self.sigmoid(y)\n        return y\n\n    def output_num(self):\n        return 1\n\n    def get_parameters(self):\n        return [{\"params\": self.parameters(), \"lr_mult\": 10, 'decay_mult': 2}]","metadata":{"execution":{"iopub.status.busy":"2023-11-30T17:55:42.710064Z","iopub.execute_input":"2023-11-30T17:55:42.710867Z","iopub.status.idle":"2023-11-30T17:55:42.722591Z","shell.execute_reply.started":"2023-11-30T17:55:42.710832Z","shell.execute_reply":"2023-11-30T17:55:42.721527Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train(model, ad_net, random_layer, train_loader, train_loader1, optimizer, optimizer_ad, epoch):\n    model.train()\n    len_source = len(train_loader)\n    len_target = len(train_loader1)\n    num_iter = min(len(train_loader), len(train_loader1))\n    total_loss = 0\n    for batch_idx in range(num_iter):\n        if batch_idx % len_source == 0:\n            iter_source = iter(train_loader)\n        if batch_idx % len_target == 0:\n            iter_target = iter(train_loader1)\n        data_source, label_source = next(iter_source)\n        if cuda:\n            data_source, label_source = data_source.cuda(), label_source.cuda()\n        data_target, label_target = next(iter_target)\n        if cuda:\n            data_target = data_target.cuda()\n        optimizer.zero_grad()\n        optimizer_ad.zero_grad()\n\n        feature_source, output_source = model(data_source)\n        feature_target, output_target = model(data_target)\n\n        feature = torch.cat((feature_source, feature_target), 0)\n        output = torch.cat((output_source, output_target), 0)\n\n        labels_target_fake = torch.max(nn.Softmax(dim=1)(output_target), 1)[1]\n        labels = torch.cat((label_source, labels_target_fake))\n\n        loss = nn.CrossEntropyLoss()(output.narrow(0, 0, data_source.size(0)), label_source)\n\n        softmax_output = nn.Softmax(dim=1)(output)\n        if epoch > 0:\n            entropy = Entropy(softmax_output)\n            loss += CDAN([feature, softmax_output], ad_net, entropy,\n                                   calc_coeff(num_iter * (epoch - 0) + batch_idx), random_layer)\n\n        mdd_loss = 0 #mdd_weight * loss_func.mdd_digit(feature, labels)\n        loss = loss + mdd_loss\n\n        total_loss += loss.data\n\n        loss.backward()\n        optimizer.step()\n        if epoch > 0:\n            optimizer_ad.step()\n        if (batch_idx + epoch * num_iter) % log_interval == 0:\n            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n                epoch, batch_idx * batch_size, num_iter * batch_size,\n                       100. * batch_idx / num_iter, loss.item()))\n    log_str = \"total_loss:{}\\n\".format(total_loss)\n    print(log_str)","metadata":{"execution":{"iopub.status.busy":"2023-11-30T17:56:39.224870Z","iopub.execute_input":"2023-11-30T17:56:39.225264Z","iopub.status.idle":"2023-11-30T17:56:39.240640Z","shell.execute_reply.started":"2023-11-30T17:56:39.225232Z","shell.execute_reply":"2023-11-30T17:56:39.239393Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def test(epoch, model, test_loader):\n    model.eval()\n    test_loss = 0\n    correct = 0\n    for data, target in test_loader:\n        if cuda:\n            data, target = data.cuda(), target.cuda()\n        feature, output = model(data)\n        test_loss += nn.CrossEntropyLoss()(output, target).item()\n        pred = output.data.cpu().max(1, keepdim=True)[1]\n        correct += pred.eq(target.data.cpu().view_as(pred)).sum().item()\n    \n    test_loss /= len(test_loader.dataset)\n    acc = 100. * correct / len(test_loader.dataset)\n    log_str = 'epoch:{},Test set: Average loss: {:.4f}, Accuracy: {}/{} ({:.4f}%)\\n'.format(epoch,\n                                                                                            test_loss, correct,\n                                                                                            len(test_loader.dataset),\n                                                                                            acc)\n\n    print(log_str)\n    return acc","metadata":{"execution":{"iopub.status.busy":"2023-11-30T17:55:44.407905Z","iopub.execute_input":"2023-11-30T17:55:44.408300Z","iopub.status.idle":"2023-11-30T17:55:44.417896Z","shell.execute_reply.started":"2023-11-30T17:55:44.408268Z","shell.execute_reply":"2023-11-30T17:55:44.416681Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = DTN()\nif cuda:\n    model = model.cuda()\nclass_num = 10\n\nif random:\n    random_layer = network.RandomLayer([model.output_num(), class_num], 500)\n    ad_net = AdversarialNetwork(500, 500)\n    if cuda:\n        random_layer.cuda()\nelse:\n    random_layer = None\n    ad_net = AdversarialNetwork(model.output_num() * class_num, 500)\nif cuda:\n    ad_net = ad_net.cuda()\noptimizer = optim.SGD(model.parameters(), lr=lr, weight_decay=0.0005, momentum=0.9)\noptimizer_ad = optim.SGD(ad_net.parameters(), lr=lr, weight_decay=0.0005, momentum=0.9)\n\nbest_model = model\nbest_acc = 0\n\nfor epoch in range(1, epochs + 1):\n    if epoch % 3 == 0:\n        for param_group in optimizer.param_groups:\n            param_group[\"lr\"] = param_group[\"lr\"] * 0.3\n\n    train(model, ad_net, random_layer, svhn_loader, mnist_loader, optimizer, optimizer_ad, epoch)\n    acc1 = test(epoch, model, svhn_test_loader)\n    acc2 = test(epoch, model, mnist_test_loader)\n    if (acc2 > best_acc):\n        best_model = model\n        best_acc = acc2\n    if epoch % 10:\n        print(\"Best Acc so far: \", best_acc)\n","metadata":{"execution":{"iopub.status.busy":"2023-11-30T17:57:17.362955Z","iopub.execute_input":"2023-11-30T17:57:17.363339Z","iopub.status.idle":"2023-11-30T19:15:03.231368Z","shell.execute_reply.started":"2023-11-30T17:57:17.363309Z","shell.execute_reply":"2023-11-30T19:15:03.229699Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"torch.save(best_model, osp.join(\"snapshot/s2m_model\", \"s2m_{}\".format(str(best_acc))))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"torch.save(best_model, 'DTNS-M_91_86.pth')","metadata":{"execution":{"iopub.status.busy":"2023-11-30T19:15:44.005085Z","iopub.execute_input":"2023-11-30T19:15:44.005485Z","iopub.status.idle":"2023-11-30T19:15:44.036528Z","shell.execute_reply.started":"2023-11-30T19:15:44.005455Z","shell.execute_reply":"2023-11-30T19:15:44.035430Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from IPython.display import FileLink\nFileLink(r'DTNS-M_91_86.pth')","metadata":{"execution":{"iopub.status.busy":"2023-11-30T19:16:04.448581Z","iopub.execute_input":"2023-11-30T19:16:04.449471Z","iopub.status.idle":"2023-11-30T19:16:04.455763Z","shell.execute_reply.started":"2023-11-30T19:16:04.449439Z","shell.execute_reply":"2023-11-30T19:16:04.454737Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}